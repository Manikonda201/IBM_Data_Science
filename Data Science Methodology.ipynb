{"nbformat_minor": 1, "cells": [{"source": "# __Data Science Methodology__ \n---", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "## Problem to Approach:\n\n__Business Understanding__ : Taking the time to seek clarification in the early stages of a project\n  * allows you to determine which data will be used to answer the core question\n  * Establishing a clearly defined question starts with understanding the _goal_ of the person asking the question.\n  * having a clearly defined question is vital because it ultimately directs the analytic approach that will be needed to address the question  \n  * ensures that the work generates the intended solution\n  * Clearly defines the problem and the needs from a business perspective.\n  * shapes the rest of the methodological steps\n  \n > Once the business problem has been clearly stated, the data scientist can define the analytic approach to solve the problem. This step entails expressing the problem in the context of statistical and machine-learning techniques, so that the entity or stakeholders with the problem can identify the most suitable techniques for the desired outcome.\n  \n__Analytic Approach__ : Identifying what type of patterns will be needed to address the question most effectively  \nIf question:\n  1. To determine probablitites of an outcome --> \n    * _Predictive Model_\n  2. To show relationships --> \n    * _Descriptive Model_\n  3. Requires a yes or no answer --> \n    * _Classification Model_\n  \n  \n  * How can you use data to answer the question?\n  * The correct approach depends on the business requirments for the model\n  \n  __Machine Learning__ : Machine Learning is a field of study that gives computers the ability to learn without being explicitly programmed.\n   * can be used to identify relationships and trends in data that might otherwise not be accessible or identified\n   \n   \n## Requirments to Collection\n\n__Data Requirments__ : Think of it as cooking with data. Must have the right ingredients (data) to meet the desired outcome.\n  * Problem that needs to be resolved is the recipe (types of data needed)\n  * Need to understand:\n    1. _what_ data is needed\n    2. How to _collect_ and source it\n    3. How to _work_ with it\n    4. How to _prepare_ the data to meet the desired outcome\n    \n__Data Collection__ : After the initnal collection of data, an assesmment is done by the data scientist to determine whether or not they have what they need\n  * Some data might be more difficult to obtain or be costly\n  * Requirments are revised and and decisions are made as to whether or not the collection requires more or less data.\n  * Techniques such as descriptive statistics and visualization can be applied to the data set, to assess the content, quality, and initial insights about the data.\n  \n > When collecting data, it is alright to defer decisions about unavailable data, and attempt to acquire it at a later stage. \n  \n  \n  * DBAs and programmers often work together to extract data from various sources, and then merge it.\n  * This allows for removing redundant data, making it available for the next stage of the methodology, which is data understanding.\n \n \n > After the data collection stage is complete, data scientists typically use __descriptive statistics__ and __visualization techniques__     to better understand the data and get acquainted with it.\n \n   Data scientists, essentially, explore the data to:\n\n   * understand its _content_\n   * assess its _quality_\n   * discover any interesting preliminary _insights_\n   * determine whether _additional data_ is necessary to fill any gaps in the data.\n   \n\n## Understanding to Preparation\n\n__Data Understanding__ : Encompasses all activities related to constructing the data set. essentially answers the question: _Is the data collected representative of the problem to be solved?_ \n\n\n__Data Preparation__ : transforming the data in a way that it can be used most effectively and addresses missing and invalid values, and removes duplicates.\n\n  * similar to washing freshly picked vegetables in so far as unwanted elements, such as dirt or imperfections, are removed. \n  * most time-consuming phase of a data science project, typically taking 70% and even up to even 90% of the overall project time.\n  * transforming data in the data preparation phase is the process of getting the data into a state where it may be easier to work with.\n\n  __Feature Engineering__: Process of using domain knowledge of the data to create features that make the machine learning algorithms work.  \n   * Features within the data are important to predictive models and will influence the results you want to achieve.\n   * Feature engineering is _critical_ when machine learning tools are being applied to analyze the data.\n  \n  > The data preparation phase sets the stage for the next steps in addressing the question. While this phase may take a while to do, if done right the results will support the project. If this is skipped over, then the outcome will not be up to par and may have you back at the drawing board.\n  \n  > It is vital to take your time in this area, and use the tools available to automate common steps to accelerate data preparation. Make sure to pay attention to the detail in this area. After all, it takes just one bad ingredient to ruin a fine meal. \n\n\n## Modeling to Evaluation\n\n__Data Modeling__ : Data Modelling focuses on developing models that are either _descriptive_ or _predictive_. \n  * Stage in which the chef (Data scientist) can sample the sauce (data/analysis) and see if it is good or needs more seasoning (adjusting)\n  * Answers the Question : In what way can the data be visualized to get te answer that we need\n\n\n  * __Descriptive Model__ : Desrcibes relationships. A descriptive model might examine things like: if a person did this, then they're likely to prefer that.\n  * __Predictive Model__ : tries to yield yes/no, or stop/go type outcomes\n    * Training Set  :\n      * used for predictive modeling\n      * a set of historical data in which the outcomes are already known. \n      * acts like a gauge to determine if the model needs to be calibrated.  \n      \n      \n  * The success of data compilation, preparation and modelling, depends on the understanding of the problem at hand, and the appropriate analytical approach being taken.\n  * Answer of the question is supported by the data\n  * The outcome is supported by the quality of the data \n    \n  > The end goal is to move the data scientist to a point where a data model can be built to answer the question.\n  \n\n__Data Evaluation__ : Allows the quality of the model to be assesed \n  * answers the Question: _Does the model uses answer the initial question_\n  * Can have two main phases:\n    1. __Diagnostic Measures:__ used to ensure if the model is working as intented \n      * If it is a _predictive model_, a _decision tree_ can be used to determine if the output of the model is alinged to the initial design\n      * If it is _descriptive model_ where relationships are being assesed, then a _testing set_ with known outcomes can be applied so the model can be refined as needed\n      \n    2. __Statistical Significance:__ Can be applied to the model to ensure that the data is being properly handled and interpreted within the model\n      * Designed to avoid unneccessary second guessing when the asnwer is revealed\n    \n  \n\n\n\n\n\n\n", "cell_type": "markdown", "metadata": {}}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "", "cell_type": "markdown", "metadata": {}}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.5", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.5", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}